---
layout: post
title: "学术报告"
subtitle: '基于即插即用适配器与多模态大模型的可解释深度伪造视频检测'
date: 2025.12.09
host: "石仲玉"
digest: "本次组会围绕深度伪造视频检测展开，分享了两篇前沿工作。第一部分介绍了一种即插即用的时空适配器与视频级混合数据增强方法，旨在低成本提升模型的泛化能力；第二部分介绍了基于多模态大模型（MLLM）的 EDVD-LLaMA，通过细粒度思维链与结构化面部指标，实现对伪造视频的精准且可解释检测。"
permalink: /:categories/:year/:month/:day/:title
categories:
  - seminar
---

## 深度伪造视频检测：从泛化性到可解释性

## 1. 任务简介

本次汇报聚焦于深度伪造视频（Deepfake Video）检测任务，重点解决当前领域面临的两大核心难题：
1.  **泛化性与成本平衡**：如何在不依赖高昂计算成本的视频模型前提下，利用强大的图像骨干网络（Backbone）实现对时序伪造特征的高效捕捉，并提升跨数据集与跨伪造方法的泛化能力 。
2.  **可解释性与幻觉抑制**：如何利用多模态大语言模型（MLLM）不仅输出真假标签，还能提供基于证据的可靠推理过程，同时解决大模型在视频检测中常见的“幻觉”问题。

## 2. 研究动机

随着 AIGC 技术的发展，深度伪造视频带来的风险日益严峻，现有方法存在以下局限：

**针对泛化性问题（Paper 1）：**
* **时序特征微弱且建模昂贵**：相比空间伪造特征，时序特征（如帧间不一致）更为微弱，且视频模型训练成本高，限制了大规模数据的利用 。
* **时空学习偏差**：现有方法往往在空间纹理与时间线索之间顾此失彼，难以兼顾 。

**针对可解释性问题（Paper 2）：**
* **缺乏透明度**：传统模型为“黑箱”，缺乏推理依据，无法满足可信应用需求 。
* **时序利用不足与幻觉**：现有基于 MLLM 的方法多关注静态图像，忽视跨帧微表情与边界伪影；且大模型容易生成脱离事实的解释（幻觉） 。

## 3. 方法设计（即插即用适配器 + 可解释大模型）

报告详细介绍了两套针对性解决方案：

**1）基于即插即用适配器的泛化检测（Paper 1: CVPR 2025）**
* **Video-level Blending (VB)**：一种数据层面的增强策略。通过从原视频提取关键点并施加微小扰动（旋转、缩放、平移），再进行仿射变形与局部融合，在数据中人工构造“面部特征漂移”伪迹，迫使模型关注跨帧不一致性 。
* **Spatiotemporal Adapter (StA)**：一种轻量级模型模块。将其插入冻结的强大图像 Backbone（如 CLIP、ViT）中，通过降维、并行的 3D 卷积（分别提取空间与时间特征）及跨注意力融合，使图像模型以极低成本获得时空建模能力 。

**2）基于多模态大模型的可解释检测（Paper 2: EDVD-LLaMA）**
* **ER-FF++ 数据集**：构建了一个包含 7362 个样本的可解释视频数据集，通过设计针对性提示词并引入人工审阅，减少数据端的幻觉 。
* **ST-SIT（时空细微信息切分）**：采用双分支结构，融合局部网格特征（捕捉伪影）与全局语义特征（保持一致性），生成包含丰富时空信息的视觉 Token 。
* **Fg-MCoT（细粒度多模态思维链）**：引入“结构化面部动态指标”（如模糊度变化、颜色分布、纹理对比度等）作为硬证据，约束大模型的推理过程 。

## 4. 实施细节

**Paper 1 实施细节：**
* **VB 流程**：逐帧检测人脸关键点 -> 对眼眉鼻口区域施加随机均匀分布的扰动 -> 生成形变图像与掩码 -> 融合生成伪造帧 。
* **StA 架构**：特征先经过线性降维，分为两路分别通过 $(1, N, N)$ 空间卷积与 $(N, 1, 1)$ 时间卷积，再经由双向 Cross-Attention 交互，最后上投影恢复维度 。

**Paper 2 实施细节：**
* **特征提取**：将视频采样为 $8 \times 9$ 帧片段。ST-SIT 模块结合 DSEncoder（局部）与 SigLIP（全局）提取特征 。
* **指标计算**：通过拉普拉斯方差计算模糊度，使用灰度共生矩阵计算纹理对比度，以及计算频域差异等，生成 JSON 格式的面部指标 $T_{fac}$ 。
* **两阶段训练**：
    * **阶段一（理由生成）**：输入时空特征与面部指标，优化模型生成推理 $R_c$ 的能力 ($L_{rle}$) 。
    * **阶段二（最终判定）**：结合问题、时空特征与生成的推理 $R_c$，进行真假分类 ($L_{ans}$) 。
    * **一致性约束**：引入 $L_{cons}$ 损失，通过 KL 散度强制推理过程与最终判断依赖相同的证据注意力 。

## 5. 实验与结论

**Paper 1 实验结果：**
* 在 CDF-v2、DFD、DFDC 等多个数据集上的测试表明，该方法（StA + VB）显著优于现有 SOTA（如 LAA-Net, LSDA） 。
* 在跨数据集与跨伪造类型（Cross Forgery）测试中表现出强大的泛化性 。
* 消融实验证明，StA 能使参数量极小的适配器在冻结的大模型上发挥显著作用，且 VB 数据增强有效提升了模型对时序异常的敏感度 。

**Paper 2 实验结果：**
* EDVD-LLaMA 在 ER-FF++ 数据集上取得了 84.75% 的准确率与 87.64% 的 AUC，大幅超越 Video-LLaVA、Video-ChatGPT 等通用视频大模型 。
* 在解释生成质量上，其 CIDEr、ROUGE-L 等文本相似度指标均最优，证明生成的解释更接近人类专家逻辑 。
* 定性分析显示，该模型能准确捕捉到皮肤纹理过度平滑、人脸关键点轨迹异常跳变等微细时空伪造痕迹，并给出令人信服的判决理由 。
[本次组会内容下载链接](https://github.com/Evangeline-J/files/blob/main/1209.pptx)