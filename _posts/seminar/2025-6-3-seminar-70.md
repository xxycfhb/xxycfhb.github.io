---
layout: post
title: "学术报告"
subtitle: '大模型推理训练与测试阶段优化技术分享'
date: 2025.6.3
host: "张仕捷"
digest: "本次组会主要分享了大模型推理训练与测试阶段优化技术"
permalink: /:categories/:year/:month/:day/:title
categories:
  - seminar
---
## 大模型推理训练与测试阶段优化技术

## 1. 传统范式对比
分析模仿学习（SFT）与纯在线强化学习（Zero-RL）的优缺点，指出前者“照搬”不擅长新题，后者易陷入局部最优

## 2. LUFFY方法
在RL中引入离策略(off-policy)示范，混合使用在线(on-policy)与离线轨迹，并通过策略塑形函数防止过早收敛、保持持续探索

## 3. LUFFY实验证明
在困难子集上，LUFFY持续提升奖励且保持高策略熵，显著优于纯在线和其他混合策略方法

## 4. TTRL（测试时RL）
通过多数投票生成伪标签，构造无监督奖励，在测试阶段动态优化推理流程，增强模型泛化能力

## 5. RLVR伪奖励探索
设计格式奖励、随机奖励、错误奖励等多种伪奖励，研究其对模型代码推理行为与性能的影响，并通过消融实验验证裁剪策略和不同RL算法的兼容性


[本次组会内容下载链接](https://github.com/Lizhizhiyi/PPT/blob/main/files/20250603.pdf)
