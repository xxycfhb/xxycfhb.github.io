---
layout: post
title: "学术报告"
subtitle: 'DeepSeek正是为王的理由'
date: 2025.2.25
host: "姚文翰"
digest: "本次组会主要分享了DeepSeek大家族的模型技术"
permalink: /:categories/:year/:month/:day/:title
categories:
  - seminar
---
## DeepSeek正是为王的理由

## 1. DeepSeek概述

## 2. 大语言模型（LLM）前置知识

## 3. DeepSeek模型家族
DeepSeek-R1-Zero
- ‌基础‌：基于671B预训练的DeepSeek-V3基础模型。
- ‌训练方法‌：使用强化学习（RL）进行训练，并引入两种类型的奖励，无SFT（监督微调）步骤。
DeepSeek-R1 
- ‌基础‌：基于DeepSeek-R1-Zero模型构建。 
- ‌优化方法‌：通过额外的SFT阶段和进一步的RL训练进行优化，提升模型的可读性和推理能力。
DeepSeek-R1-Distill 
- ‌基础‌：使用DeepSeek-R1的输出数据对Qwen和Llama等小型模型进行微调。 
- ‌目的‌：提升小型模型的推理能力，提供高效的推理解决方案。

## 4. DeepSeek的技术原理

## 5. DeepSeek的实际应用与效果





[本次组会内容下载链接](https://github.com/Lizhizhiyi/PPT/blob/main/files/20250225.pdf)
